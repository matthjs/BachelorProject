{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![Image Title](./torchrl.png)\n",
    "\n",
    "TorchRL overview. The left side showcases the key components of the library, demonstrating the data flow with TensorDict instances passing between modules. On the right side, a code snippet is provided as a toy example, illustrating the training of DDPG. The script provides users with full control over the algorithm’s hyperparameters, offering a concise yet comprehensive solution. Still, replacing a minimal number of components in the script enables a seamless transition to another similar algorithm, like SAC or REDQ. Instead of a custom train loop there are is also a Trainer class available.\n",
    "\n",
    "The key concept compared to other RL libraries is that TorchRL is highly modular, providing well-integrated, standalone components."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5842355efdffef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The TensorDict\n",
    "\n",
    "The `TensorDict` is the core PyTorch primitive that is used in `torchrl`.  `TensorDict`s are used as a communication object for interaction between the independent components of the `torchrl` library.\n",
    "\n",
    "*  An `TensorDict` is a dictionary-like object that stores tensors(-like) objects. It includes additional features that optimize its use with PyTorch.\n",
    "*  As the function signatures are generic, it eliminates the challenge of accommodating different data formats."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e462081d3a09cd"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:30.615854300Z",
     "start_time": "2024-04-21T12:47:28.103574100Z"
    }
   },
   "id": "6ae65cafb2ec4bd2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "tensordict = TensorDict(\n",
    "    source={\n",
    "        \"key 1\": torch.zeros(batch_size, 3),\n",
    "        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")\n",
    "print(tensordict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:30.630767300Z",
     "start_time": "2024-04-21T12:47:30.616863800Z"
    }
   },
   "id": "2a04c2ec2a47e850"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can index a TensorDict as well as query keys."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9719e3979328608"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tensordict[2])\n",
    "print(tensordict[\"key 1\"] is tensordict.get(\"key 1\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:30.641836200Z",
     "start_time": "2024-04-21T12:47:30.630767300Z"
    }
   },
   "id": "dcecc8edbdefa475"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb78a76f4a6e9dab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Wrappers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c542c165fe9964a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torchrl` offers wrappers for environment from different libraries. For `gymnasium` environments there is the `GymEnv` wrapper. This wrapper allows for a few things such as converting relevant information such as trajectories into `TensorDict`s and allowing for multiple instances of the environment to be run in parallel, which can speed up training. The most important thing to remember is that in `torchrl`, environment methods read and write `TensorDict` instances. This means that methods like `env.reset()` and `env.rand_action(state)` now return `TensorDict`s."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8a9eab0fb9be04"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def fetch_device() -> str:\n",
    "    \"\"\"\n",
    "    Returns 'cuda' if GPU is available otherwise 'cpu'.\n",
    "    Can be used to determine where to move tensors to.\n",
    "    :return: string 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:30.712883600Z",
     "start_time": "2024-04-21T12:47:30.643842200Z"
    }
   },
   "id": "1c77a8dff79b0e3c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "df1e2c23980e207a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\", device=fetch_device())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.115470200Z",
     "start_time": "2024-04-21T12:47:30.653745600Z"
    }
   },
   "id": "5448d17a505d6fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "reset = env.reset()\n",
    "print(reset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.123464400Z",
     "start_time": "2024-04-21T12:47:37.118466700Z"
    }
   },
   "id": "7f04a0d59791af03"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "reset_with_action = env.rand_action(reset)\n",
    "print(reset_with_action)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.140606200Z",
     "start_time": "2024-04-21T12:47:37.136465Z"
    }
   },
   "id": "f9672b83bbc5fb09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Environment related information are now all converted to PyTorch tensors. We can see for example that the action is a tensor instead of just a float."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae5025a9ad84d449"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1696], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(reset_with_action['action'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.217617400Z",
     "start_time": "2024-04-21T12:47:37.140606200Z"
    }
   },
   "id": "693fbb3dd9ff06e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at what the `step()` method now outputs. It returns a new TensorDict containing:\n",
    "* `action`, what action did our agent perform.\n",
    "* `done`, as is defined in Gymnasium environments.\n",
    "* `next`, contains another tensordict with `done`, `observation`, `reward`, `terminated`, `truncated` fields. Naturally all the elements are tensors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64fb17a81b9c277"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.218626600Z",
     "start_time": "2024-04-21T12:47:37.163121200Z"
    }
   },
   "id": "64132847b5cf4b3b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4657,  0.8850,  1.4465], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Extracting the next state tensor\n",
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data['next']['observation'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.219177100Z",
     "start_time": "2024-04-21T12:47:37.168200400Z"
    }
   },
   "id": "7ec465c9fb8ade30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This format returned by the `step()` method is called `TED` (TorchRL Episode Data format). This format is crucial for the seamless integration and functioning of various components within TorchRL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "730b520df3ec777c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last bit of information you need to run a rollout in the environment is how to bring that \"next\" entry at the root to perform the next step. TorchRL provides a dedicated step_mdp() function that does just that: it filters out the information you won’t need and delivers a data structure corresponding to your observation after a step in the Markov Decision Process, or MDP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb87084814558cc"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87c463de47acf81"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import step_mdp\n",
    "\n",
    "data = step_mdp(stepped_data)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.221144Z",
     "start_time": "2024-04-21T12:47:37.181715700Z"
    }
   },
   "id": "11e00b8356d52890"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collecting Environment Interaction Data with Rollouts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e7c3aed266c8fbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Writing down those three steps (computing an action, making a step, moving in the MDP) can be a bit tedious and repetitive. Fortunately, TorchRL provides a nice `rollout()` function that allows you to run them in a closed loop at will.\n",
    "The `rollout` method functions across all kinds of use-cases (single-agent, parallel, multi-agent).\n",
    "\n",
    "`TensorDict` will automatically check if the index you provided is a key (in which case we index along the key-dimension) or a spatial index (e.g., [3])."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9694cb4dc00672cc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "# Gives a tensordict of the same shape as is returned by the `step()` method but with\n",
    "# a batch size of 10.\n",
    "rollout = env.rollout(max_steps=10)    # By default executes with random policy.\n",
    "print(rollout)\n",
    "\n",
    "# rollout[3] gives third trajectory."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.223603200Z",
     "start_time": "2024-04-21T12:47:37.185722Z"
    }
   },
   "id": "d536088cff717d0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environment Transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81bde004940bd9b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Often you want to modify the output of the environment to better suit your requirements. This can include:\n",
    "* Monitor the number of steps executed since the last reset.\n",
    "* Resize images, stack consecutive observations together."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7316ca9662496833"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                step_count: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import StepCounter, TransformedEnv\n",
    "\n",
    "transformed_env = TransformedEnv(env, StepCounter(max_steps=10))\n",
    "rollout = transformed_env.rollout(max_steps=100)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.375964600Z",
     "start_time": "2024-04-21T12:47:37.252826700Z"
    }
   },
   "id": "8fe88d5e65eec495"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our environment now has one more entry, \"step_count\" that tracks the number of steps since the last reset. Given that we passed the optional argument max_steps=10 to the transform constructor, we also truncated the trajectory after 10 steps (not completing a full rollout of 100 steps like we asked with the rollout call). We can see that the trajectory was truncated by looking at the truncated entry:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6617daf9f9b2832"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rollout[\"next\", \"truncated\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.377504100Z",
     "start_time": "2024-04-21T12:47:37.258368100Z"
    }
   },
   "id": "d7cad49daf8d3619"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various other transformations are possible. For example, `ParallelEnv` allows you to run multiple copies of one same (or different!) environments on multiple processes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f4ed6e21af4185"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TorchRL modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebac8d216d57ec37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorDictModules\n",
    "\n",
    "Similar to how environments interact with instances of TensorDict, the modules used to represent policies and value functions also do the same. The core idea is simple: encapsulate a standard Module (or any other function) within a class that knows which entries need to be read and passed to the module, and then records the results with the assigned entries. To illustrate this, we will use the simplest policy possible: a deterministic map from the observation space to the action space. For maximum generality, we will use a LazyLinear module with the Pendulum environment we instantiated in the previous tutorial.\n",
    "\n",
    "Primitives like `TensorDictModule` and `TensorDictSequential` allow one to design complex PyTorch operations in an explicit and programmable way. `TensorDictModule` wraps PyTorch modules, transforming them into tensordict-compatible objects for effortless integration into the TorchRL framework. Concurrently, `TensorDictSequential` concatenates `TensorDictModule`s, functioning similar to `nn.Sequential`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59a23db80bcf4b83"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\", device=fetch_device())\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1], device=fetch_device())   # Lazy module allows us to bypass the need to fetch the shape of the observation space, as the module will automatically determine it.\n",
    "\n",
    "# This TensorDictModule describes that our policy is a function from `observation` to `action`, where the function is the previously defined module.\n",
    "policy = TensorDictModule(\n",
    "    module,                     # Function (approximate) for our policy\n",
    "    in_keys=[\"observation\"],    # dictionary key for input\n",
    "    out_keys=[\"action\"],        # dictionary key for action\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:37.377504100Z",
     "start_time": "2024-04-21T12:47:37.264347500Z"
    }
   },
   "id": "8a565a793e670c89"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:38.986509900Z",
     "start_time": "2024-04-21T12:47:37.281132400Z"
    }
   },
   "id": "172714e5dcc32db9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TorchRL also provides regular modules that can be used without recurring to tensordict features. The two most common networks you will encounter are the `MLP` and the `ConvNet` (CNN) modules. We can substitute our policy module with one of these:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87f9c45d58ace7a"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, Actor\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=torch.nn.Tanh,\n",
    "    device=fetch_device()\n",
    ")\n",
    "policy = Actor(module)    # Actor is shorthand for the previously defined TensorDictModule.\n",
    "rollout = env.rollout(max_steps=10, policy=policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.013974600Z",
     "start_time": "2024-04-21T12:47:38.991031100Z"
    }
   },
   "id": "772b42eee38f0560"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Probabilistic Policies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fecbf4d21dbd8cfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policy-optimization sota-implementations like PPO require the policy to be stochastic: unlike in the examples above, the module now encodes a map from the observation space to a parameter space encoding a distribution over the possible actions. TorchRL facilitates the design of such modules by grouping under a single class the various operations such as building the distribution from the parameters, sampling from that distribution and retrieving the log-probability. Here, we’ll be building an actor that relies on a regular normal distribution using three components:*\n",
    "* An `MLP` backbone reading observations of size [3] and outputting a single tensor of size [2]\n",
    "* A `NormalParamExtractor` module that will split this output on two chunks, a mean and a standard deviation of size [1];\n",
    "* A `ProbabilisticActor` that will read those parameters as in_keys, create a distribution with them and populate our tensordict with samples and log-probabilities.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5c659b4a2210d2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e95815d7c94c401a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        loc: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        scale: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "backbone = MLP(in_features=3, out_features=2, device=fetch_device())\n",
    "extractor = NormalParamExtractor()\n",
    "module = torch.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],    # Parameters of Gaussian distribution.\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "\n",
    "# Since we asked for it during the construction of the actor, the log-probability of the actions given the distribution at that time is also written. This is necessary for sota-implementations like PPO.\n",
    "\n",
    "# The parameters of the distribution are returned within the output tensordict too under the \"loc\" and \"scale\" entries."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.071564400Z",
     "start_time": "2024-04-21T12:47:39.013974600Z"
    }
   },
   "id": "d8537d24ec453f40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can control the sampling of the action to use the expected value or other properties of the distribution instead of using random samples if your application requires it. This can be controlled via the set_exploration_type() function:\n",
    "```\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # takes the mean as action\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Samples actions according to the dist\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c549dd73b58f599"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Epsilon-Greedy Action Selection for Value-Based algorithms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f181e0cf5fb0dc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic policies like this somewhat naturally trade off exploration and exploitation, but deterministic policies won’t. Fortunately, TorchRL can also falliate to this with its exploration modules. We will take the example of the EGreedyModule exploration module (check also AdditiveGaussianWrapper and OrnsteinUhlenbeckProcessWrapper). To see this module in action, let’s revert to a deterministic policy:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff6c2ead03d6d93"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.modules import EGreedyModule\n",
    "\n",
    "policy = Actor(MLP(3, 1, num_cells=[32, 64], device=fetch_device()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.072109800Z",
     "start_time": "2024-04-21T12:47:39.051179500Z"
    }
   },
   "id": "ead20be6768a190e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our $\\epsilon$-greedy exploration module will usually be customized with a number of annealing frames and an initial value for the parameter. A value of means that every action taken is random, while means that there is no exploration at all. To anneal (i.e., decrease) the exploration factor, a call to step() is required (see the last tutorial for an example).\n",
    "\n",
    "Because it must be able to sample random actions in the action space, the EGreedyModule must be equipped with the action_space from the environment to know what strategy to use to sample actions randomly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faafddad71317737"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "exploration_module = EGreedyModule(\n",
    "    spec=env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.074013700Z",
     "start_time": "2024-04-21T12:47:39.063427900Z"
    }
   },
   "id": "deea37db1c0f6ec9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To build our explorative policy, we only had to concatenate the deterministic policy module with the exploration module within a TensorDictSequential module (which is the analogous to Sequential in the tensordict realm)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7acade2f1dc835d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # Turns off exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Turns on exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.135177500Z",
     "start_time": "2024-04-21T12:47:39.065039300Z"
    }
   },
   "id": "9d966a3d6090c062"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Value actors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349ae9ff4e285d2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In some settings, the policy isn’t a standalone module but is constructed on top of another module. This is the case with Q-Value actors. In short, these actors require an estimate of the action value (most of the time discrete) and will greedily pick up the action with the highest value. In some settings (finite discrete action space and finite discrete state space), one can just store a 2D table of state-action pairs and pick up the action with the highest value. The innovation brought by DQN was to scale this up to continuous state spaces by utilizing a neural network to encode for the Q(s, a) value map. Let’s consider another environment with a discrete action space for a clearer understanding:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adcfe98832b647d4"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotDiscreteTensorSpec(\n",
      "    shape=torch.Size([2]),\n",
      "    space=DiscreteBox(n=2),\n",
      "    device=cuda,\n",
      "    dtype=torch.int64,\n",
      "    domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(\"CartPole-v1\", device=fetch_device())\n",
    "print(env.action_spec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.135177500Z",
     "start_time": "2024-04-21T12:47:39.119368500Z"
    }
   },
   "id": "51eb306747f53dba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We build a value network that produces one value per action when it reads a state from the environment:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d6dae816bde4bd"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "value_net = TensorDictModule(\n",
    "    MLP(out_features=num_actions, num_cells=[32, 32], device=fetch_device()),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_value\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.167920700Z",
     "start_time": "2024-04-21T12:47:39.132442400Z"
    }
   },
   "id": "1fd8d4eb2ce04fdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily build our Q-Value actor by adding a QValueModule after our value network:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e65dccbbc5d10974"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from torchrl.modules import QValueModule\n",
    "\n",
    "policy = TensorDictSequential(\n",
    "    value_net,  # writes action values in our tensordict\n",
    "    QValueModule(\n",
    "        action_space=env.action_spec\n",
    "    ),  # Reads the \"action_value\" entry by default\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.170679300Z",
     "start_time": "2024-04-21T12:47:39.138595800Z"
    }
   },
   "id": "785e15cbe547640c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s check it out! We run the policy for a couple of steps and look at the output. We should find an \"action_value\" as well as a \"chosen_action_value\" entries in the rollout that we obtain:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc622aaccb9b9c7c"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 2]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        action_value: Tensor(shape=torch.Size([3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=3, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.225082700Z",
     "start_time": "2024-04-21T12:47:39.148766600Z"
    }
   },
   "id": "4a9993050fe6726e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because it relies on the argmax operator, this policy is deterministic. During data collection, we will need to explore the environment. For that, we are using the EGreedyModule once again:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4389d6f0da42ac32"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "policy_explore = TensorDictSequential(policy, EGreedyModule(env.action_spec))\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T12:47:39.225605800Z",
     "start_time": "2024-04-21T12:47:39.174176500Z"
    }
   },
   "id": "bfb850c3e9486c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b316530c2f6af509"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In TorchRL, we try to treat optimization as it is custom to do in PyTorch, using dedicated loss modules which are designed with the sole purpose of optimizing the model. This approach efficiently decouples the execution of the policy from its training and allows us to design training loops that are similar to what can be found in traditional supervised learning examples.\n",
    "\n",
    "The typical training loop therefore looks like this:\n",
    "```\n",
    ">>> for i in range(n_collections):\n",
    "...     data = get_next_batch(env, policy)\n",
    "...     for j in range(n_optim):\n",
    "...         loss = loss_fn(data)\n",
    "...         loss.backward()\n",
    "...         optim.step()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a97130f524451e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In TorchRL reinforcement learning objective functions are encapsulated as loss modules. To build a loss module one needs a set of networks defined as class `tensordict.nn.TensorDictModule`. For instance the DDPG loss requires a deterministic map from observation space to action space and a value network that predicts the value of a state-action pair. The DDPG loss will attempt to find the policy parameters that output actions that maximize the value for a given state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca1d94ff6d94095d"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")\n",
    "\n",
    "from torchrl.modules import Actor, MLP, ValueOperator\n",
    "from torchrl.objectives import DDPGLoss\n",
    "\n",
    "n_obs = env.observation_spec[\"observation\"].shape[-1]\n",
    "n_act = env.action_spec.shape[-1]\n",
    "actor = Actor(MLP(in_features=n_obs, out_features=n_act, num_cells=[32, 32]))\n",
    "value_net = ValueOperator(\n",
    "    MLP(in_features=n_obs + n_act, out_features=1, num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\", \"action\"],\n",
    ")\n",
    "\n",
    "ddpg_loss = DDPGLoss(actor_network=actor, value_network=value_net)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:03:11.390633100Z",
     "start_time": "2024-04-21T13:03:11.310862500Z"
    }
   },
   "id": "46447369a3041e6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that is it! Our loss module can now be run with data coming from the environment (we omit exploration, storage and other features to focus on the loss functionality):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a3679861f2e628"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        td_error: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=100, policy=actor)\n",
    "loss_vals = ddpg_loss(rollout)\n",
    "print(loss_vals)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:04:33.924115600Z",
     "start_time": "2024-04-21T13:04:33.859085700Z"
    }
   },
   "id": "6074cb016d54f986"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the value we received from the loss isn’t a single scalar but a dictionary containing multiple losses.\n",
    "\n",
    "The reason is simple: because more than one network may be trained at a time, and since some users may wish to separate the optimization of each module in distinct steps, TorchRL’s objectives will return dictionaries containing the various loss components.\n",
    "\n",
    "This format also allows us to pass metadata along with the loss values. In general, we make sure that only the loss values are differentiable such that you can simply sum over the values of the dictionary to obtain the total loss. If you want to make sure you’re fully in control of what is happening, you can sum over only the entries which keys start with the \"loss_\" prefix:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba5ffd215a6e0ae"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for key, val in loss_vals.items():\n",
    "    if key.startswith(\"loss_\"):\n",
    "        total_loss += val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:14:50.509191300Z",
     "start_time": "2024-04-21T13:14:50.462189100Z"
    }
   },
   "id": "f967bc9e3e4da31c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a LossModule"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fedf770e78dad72a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given all this, training the modules is not so different from what would be done in any other training loop. Because it wraps the modules, the easiest way to get the list of trainable parameters is to query the parameters() method.\n",
    "\n",
    "We’ll need an optimizer (or one optimizer per module if that is your choice)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ff8f53452d96024"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optim = Adam(ddpg_loss.parameters())\n",
    "total_loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:15:47.347043500Z",
     "start_time": "2024-04-21T13:15:45.942836100Z"
    }
   },
   "id": "97553924c9a1f52d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another important aspect to consider is the presence of target parameters in off-policy sota-implementations like DDPG. Target parameters typically represent a delayed or smoothed version of the parameters over time, and they play a crucial role in value estimation during policy training. Utilizing target parameters for policy training often proves to be significantly more efficient compared to using the current configuration of value network parameters. Generally, managing target parameters is handled by the loss module, relieving users of direct concern. However, it remains the user’s responsibility to update these values as necessary based on specific requirements. TorchRL offers a couple of updaters, namely HardUpdate and SoftUpdate, which can be easily instantiated without requiring in-depth knowledge of the underlying mechanisms of the loss module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "348bf0ee3987d704"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from torchrl.objectives import SoftUpdate\n",
    "\n",
    "updater = SoftUpdate(ddpg_loss, eps=0.99)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:48.898765100Z",
     "start_time": "2024-04-21T13:18:48.852250300Z"
    }
   },
   "id": "29d1080abe50ef44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your training loop, you will need to update the target parameters at each optimization step or each collection step:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4401fb283b60c88"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "updater.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:19:09.250208500Z",
     "start_time": "2024-04-21T13:19:09.206208Z"
    }
   },
   "id": "504b9c598eb37472"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Collection and Storage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d2f05ebaf37e9d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TorchRL approaches the problem of dataloading in a similar manner to `DataLoader`s and the like in supervised learning, although it is surprisingly unique in the ecosystem of RL libraries. TorchRL’s dataloaders are referred to as DataCollectors. Most of the time, data collection does not stop at the collection of raw data, as the data needs to be stored temporarily in a buffer (or equivalent structure for on-policy sota-implementations) before being consumed by the loss module. This tutorial will explore these two classes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27a09c19f8a8713e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Collectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c13f2f112972f6b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A collector is a class responsible for:\n",
    "* executing your policy within the environment,\n",
    "* resetting the environment when necessary,\n",
    "* and providing batches of a predefined size.\n",
    "\n",
    "Unlike the `rollout()` method collectors do not reset between consecutive batches of data. Consequently, two successive batches of data may contain elements from the same trajectory.\n",
    "The basic arguments you need to pass to your collector are the size of the batches you want to collect (frames_per_batch), the length (possibly infinite) of the iterator, the policy and the environment. For simplicity, we will use a dummy, random policy in this example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec80e5bd1d090648"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import GymEnv\n",
    "from torchrl.envs.utils import RandomPolicy\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\", device=fetch_device())\n",
    "env.set_seed(0)\n",
    "\n",
    "policy = RandomPolicy(env.action_spec)\n",
    "collector = SyncDataCollector(env, policy, frames_per_batch=200, total_frames=-1, device=fetch_device())    # -1 -> never ending collector."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:26:09.264523200Z",
     "start_time": "2024-04-21T13:26:09.183110400Z"
    }
   },
   "id": "37f2c8d7250308a6"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([200, 2]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([200]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([200, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([200, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([200, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([200]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "for data in collector:\n",
    "    print(data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:26:10.993473600Z",
     "start_time": "2024-04-21T13:26:10.580173600Z"
    }
   },
   "id": "1dcf4bf6701a9770"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is augmented with some collector-specific metadata grouped in a `\"collector\"` sub-tensordict. This is useful for keeping track of trajectory ids."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c251948973c154ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data collectors are very useful when it comes to coding state-of-the-art sota-implementations, as performance is usually measured by the capability of a specific technique to solve a problem in a given number of interactions with the environment (the total_frames argument in the collector). For this reason, most training loops in our examples look like this:\n",
    "```\n",
    ">>> for data in collector:\n",
    "...     # your algorithm here\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b3dd60db6639ba0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Replay Buffers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0ea841b88885d9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have explored how to collect data, we would like to know how to store it. In RL, the typical setting is that the data is collected, stored temporarily and cleared after a little while given some heuristic: first-in first-out or other. A typical pseudo-code (for off-policy algorithm?) would look like this:\n",
    "```\n",
    ">>> for data in collector:\n",
    "...     storage.store(data)\n",
    "...     for i in range(n_optim):\n",
    "...         sample = storage.sample()\n",
    "...         loss_val = loss_fn(sample)\n",
    "...         loss_val.backward()\n",
    "...         optim.step() # etc\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b7677d7aed9bcb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parent class that stores the data in TorchRL is referred to as ReplayBuffer. TorchRL’s replay buffers are composable: you can edit the storage type, their sampling technique, the writing heuristic or the transforms applied to them. We will leave the fancy stuff for a dedicated in-depth tutorial. The generic replay buffer only needs to know what storage it has to use. In general, we recommend a TensorStorage subclass, which will work fine in most cases. We’ll be using LazyMemmapStorage in this tutorial, which enjoys two nice properties: first, being “lazy”, you don’t need to explicitly tell it what your data looks like in advance. Second, it uses MemoryMappedTensor as a backend to save your data on disk in an efficient way. The only thing you need to know is how big you want your buffer to be."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64452c62eb182def"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import LazyMemmapStorage, ReplayBuffer\n",
    "\n",
    "buffer = ReplayBuffer(storage=LazyMemmapStorage(max_size=1000, device=fetch_device()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:35:25.507537200Z",
     "start_time": "2024-04-21T13:35:25.497531500Z"
    }
   },
   "id": "47094f6ec5307d69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Populating the buffer can be done via the add() (single element) or extend() (multiple elements) methods. Using the data we just collected, we initialize and populate the buffer in one go:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d5ae6a2cf0fb993"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "indices = buffer.extend(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:35:27.743262800Z",
     "start_time": "2024-04-21T13:35:27.564958100Z"
    }
   },
   "id": "ef4012cffde57a64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check that the buffer now has the same number of elements than what we got from the collector:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3de83d4bcbf460d4"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "assert len(buffer) == collector.frames_per_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:35:29.022267600Z",
     "start_time": "2024-04-21T13:35:29.010245800Z"
    }
   },
   "id": "575d1384004fa306"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only thing left to know is how to gather data from the buffer. Naturally, this relies on the sample() method. Because we did not specify that sampling had to be done without repetitions, it is not guaranteed that the samples gathered from our buffer will be unique:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cbab03c77f7a044"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([30, 2]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([30]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([30, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([30]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([30, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([30, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([30]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "sample = buffer.sample(batch_size=30)\n",
    "print(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:35:30.753407300Z",
     "start_time": "2024-04-21T13:35:30.698302900Z"
    }
   },
   "id": "aebc41c809f2ff3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
