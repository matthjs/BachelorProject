{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TorchRL\n",
    " A modular, primitive-first, python-first PyTorch library for Reinforcement Learning. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb316e8157460cbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Image Title](./torchrl.png)\n",
    "\n",
    "TorchRL overview. The left side showcases the key components of the library, demonstrating the data flow with TensorDict instances passing between modules. On the right side, a code snippet is provided as a toy example, illustrating the training of DDPG. The script provides users with full control over the algorithm’s hyperparameters, offering a concise yet comprehensive solution. Still, replacing a minimal number of components in the script enables a seamless transition to another similar algorithm, like SAC or REDQ. Instead of a custom train loop there are is also a Trainer class available.\n",
    "\n",
    "The key concept compared to other RL libraries is that TorchRL is highly modular, providing well-integrated, standalone components."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5842355efdffef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The TensorDict\n",
    "\n",
    "The `TensorDict` is the core PyTorch primitive that is used in `torchrl`.  `TensorDict`s are used as a communication object for interaction between the independent components of the `torchrl` library.\n",
    "\n",
    "*  An `TensorDict` is a dictionary-like object that stores tensors(-like) objects. It includes additional features that optimize its use with PyTorch.\n",
    "*  As the function signatures are generic, it eliminates the challenge of accommodating different data formats."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e462081d3a09cd"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:28:03.036374300Z",
     "start_time": "2024-04-13T14:28:02.989499300Z"
    }
   },
   "id": "6ae65cafb2ec4bd2"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "tensordict = TensorDict(\n",
    "    source={\n",
    "        \"key 1\": torch.zeros(batch_size, 3),\n",
    "        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n",
    "    },\n",
    "    batch_size=[batch_size],\n",
    ")\n",
    "print(tensordict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:28:11.920621500Z",
     "start_time": "2024-04-13T14:28:11.857309900Z"
    }
   },
   "id": "2a04c2ec2a47e850"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can index a TensorDict as well as query keys."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9719e3979328608"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        key 1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        key 2: Tensor(shape=torch.Size([5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tensordict[2])\n",
    "print(tensordict[\"key 1\"] is tensordict.get(\"key 1\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:28:34.439976400Z",
     "start_time": "2024-04-13T14:28:34.377024100Z"
    }
   },
   "id": "dcecc8edbdefa475"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fb78a76f4a6e9dab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Wrappers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c542c165fe9964a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torchrl` offers wrappers for environment from different libraries. For `gymnasium` environments there is the `GymEnv` wrapper. This wrapper allows for a few things such as converting relevant information such as trajectories into `TensorDict`s and allowing for multiple instances of the environment to be run in parallel, which can speed up training. The most important thing to remember is that in `torchrl`, environment methods read and write `TensorDict` instances. This means that methods like `env.reset()` and `env.rand_action(state)` now return `TensorDict`s."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8a9eab0fb9be04"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def fetch_device() -> str:\n",
    "    \"\"\"\n",
    "    Returns 'cuda' if GPU is available otherwise 'cpu'.\n",
    "    Can be used to determine where to move tensors to.\n",
    "    :return: string 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:24:14.305636Z",
     "start_time": "2024-04-13T13:24:14.274384300Z"
    }
   },
   "id": "1c77a8dff79b0e3c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "df1e2c23980e207a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\", device=fetch_device())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:24:20.056544900Z",
     "start_time": "2024-04-13T13:24:20.046538200Z"
    }
   },
   "id": "5448d17a505d6fa1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "reset = env.reset()\n",
    "print(reset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:24:21.411872800Z",
     "start_time": "2024-04-13T13:24:21.348943900Z"
    }
   },
   "id": "7f04a0d59791af03"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "reset_with_action = env.rand_action(reset)\n",
    "print(reset_with_action)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:24:23.962622500Z",
     "start_time": "2024-04-13T13:24:23.914027300Z"
    }
   },
   "id": "f9672b83bbc5fb09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Environment related information are now all converted to PyTorch tensors. We can see for example that the action is a tensor instead of just a float."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae5025a9ad84d449"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3574], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(reset_with_action['action'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:24:27.651626600Z",
     "start_time": "2024-04-13T13:24:27.587073400Z"
    }
   },
   "id": "693fbb3dd9ff06e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at what the `step()` method now outputs. It returns a new TensorDict containing:\n",
    "* `action`, what action did our agent perform.\n",
    "* `done`, as is defined in Gymnasium environments.\n",
    "* `next`, contains another tensordict with `done`, `observation`, `reward`, `terminated`, `truncated` fields. Naturally all the elements are tensors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64fb17a81b9c277"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:31:13.210020100Z",
     "start_time": "2024-04-13T13:31:13.147506900Z"
    }
   },
   "id": "64132847b5cf4b3b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9552, -0.2959, -3.1185], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Extracting the next state tensor\n",
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data['next']['observation'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:31:46.140688800Z",
     "start_time": "2024-04-13T13:31:46.109001900Z"
    }
   },
   "id": "7ec465c9fb8ade30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This format returned by the `step()` method is called `TED` (TorchRL Episode Data format). This format is crucial for the seamless integration and functioning of various components within TorchRL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "730b520df3ec777c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last bit of information you need to run a rollout in the environment is how to bring that \"next\" entry at the root to perform the next step. TorchRL provides a dedicated step_mdp() function that does just that: it filters out the information you won’t need and delivers a data structure corresponding to your observation after a step in the Markov Decision Process, or MDP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb87084814558cc"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87c463de47acf81"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import step_mdp\n",
    "\n",
    "data = step_mdp(stepped_data)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:35:20.463092600Z",
     "start_time": "2024-04-13T13:35:20.400171700Z"
    }
   },
   "id": "11e00b8356d52890"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collecting Environment Interaction Data with Rollouts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e7c3aed266c8fbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Writing down those three steps (computing an action, making a step, moving in the MDP) can be a bit tedious and repetitive. Fortunately, TorchRL provides a nice `rollout()` function that allows you to run them in a closed loop at will.\n",
    "The `rallout` method functions across all kinds of use-cases (single-agent, parallel, multi-agent).\n",
    "\n",
    "`TensorDict` will automatically check if the index you provided is a key (in which case we index along the key-dimension) or a spatial index (e.g., [3])."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9694cb4dc00672cc"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "# Gives a tensordict of the same shape as is returned by the `step()` method but with\n",
    "# a batch size of 10.\n",
    "rollout = env.rollout(max_steps=10)    # By default executes with random policy.\n",
    "print(rollout)\n",
    "\n",
    "# rollout[3] gives third trajectory."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:40:03.575136400Z",
     "start_time": "2024-04-13T13:40:03.399761500Z"
    }
   },
   "id": "d536088cff717d0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3682e6684d8756"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environment Transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81bde004940bd9b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Often you want to modify the output of the environment to better suit your requirements. This can include:\n",
    "* Monitor the number of steps executed since the last reset.\n",
    "* Resize images, stack consecutive observations together."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7316ca9662496833"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                step_count: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        step_count: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import StepCounter, TransformedEnv\n",
    "\n",
    "transformed_env = TransformedEnv(env, StepCounter(max_steps=10))\n",
    "rollout = transformed_env.rollout(max_steps=100)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:49:12.366937200Z",
     "start_time": "2024-04-13T13:49:12.243499200Z"
    }
   },
   "id": "8fe88d5e65eec495"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our environment now has one more entry, \"step_count\" that tracks the number of steps since the last reset. Given that we passed the optional argument max_steps=10 to the transform constructor, we also truncated the trajectory after 10 steps (not completing a full rollout of 100 steps like we asked with the rollout call). We can see that the trajectory was truncated by looking at the truncated entry:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6617daf9f9b2832"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(rollout[\"next\", \"truncated\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:50:10.998795700Z",
     "start_time": "2024-04-13T13:50:10.943643200Z"
    }
   },
   "id": "d7cad49daf8d3619"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Various other transformations are possible. For example, `ParallelEnv` allows you to run multiple copies of one same (or different!) environments on multiple processes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f4ed6e21af4185"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TorchRL modules"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebac8d216d57ec37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorDictModules\n",
    "\n",
    "Similar to how environments interact with instances of TensorDict, the modules used to represent policies and value functions also do the same. The core idea is simple: encapsulate a standard Module (or any other function) within a class that knows which entries need to be read and passed to the module, and then records the results with the assigned entries. To illustrate this, we will use the simplest policy possible: a deterministic map from the observation space to the action space. For maximum generality, we will use a LazyLinear module with the Pendulum environment we instantiated in the previous tutorial.\n",
    "\n",
    "Primitives like `TensorDictModule` and `TensorDictSequential` allow one to design complex PyTorch operations in an explicit and programmable way. `TensorDictModule` wraps PyTorch modules, transforming them into tensordict-compatible objects for effortless integration into the TorchRL framework. Concurrently, `TensorDictSequential` concatenates `TensorDictModule`s, functioning similar to `nn.Sequential`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59a23db80bcf4b83"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\", device=fetch_device())\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1], device=fetch_device())   # Lazy module allows us to bypass the need to fetch the shape of the observation space, as the module will automatically determine it.\n",
    "\n",
    "# This TensorDictModule describes that our policy is a function from `observation` to `action`, where the function is the previously defined module.\n",
    "policy = TensorDictModule(\n",
    "    module,                     # Function (approximate) for our policy\n",
    "    in_keys=[\"observation\"],    # dictionary key for input\n",
    "    out_keys=[\"action\"],        # dictionary key for action\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:58:01.140053100Z",
     "start_time": "2024-04-13T13:58:01.130790800Z"
    }
   },
   "id": "8a565a793e670c89"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T13:58:02.724247500Z",
     "start_time": "2024-04-13T13:58:02.565065100Z"
    }
   },
   "id": "172714e5dcc32db9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TorchRL also provides regular modules that can be used without recurring to tensordict features. The two most common networks you will encounter are the `MLP` and the `ConvNet` (CNN) modules. We can substitute our policy module with one of these:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b87f9c45d58ace7a"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, Actor\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    "    activation_class=torch.nn.Tanh,\n",
    "    device=fetch_device()\n",
    ")\n",
    "policy = Actor(module)    # Actor is shorthand for the previously defined TensorDictModule.\n",
    "rollout = env.rollout(max_steps=10, policy=policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:01:10.243576500Z",
     "start_time": "2024-04-13T14:01:10.224639700Z"
    }
   },
   "id": "772b42eee38f0560"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Probabilistic Policies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fecbf4d21dbd8cfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policy-optimization sota-implementations like PPO require the policy to be stochastic: unlike in the examples above, the module now encodes a map from the observation space to a parameter space encoding a distribution over the possible actions. TorchRL facilitates the design of such modules by grouping under a single class the various operations such as building the distribution from the parameters, sampling from that distribution and retrieving the log-probability. Here, we’ll be building an actor that relies on a regular normal distribution using three components:*\n",
    "* An `MLP` backbone reading observations of size [3] and outputting a single tensor of size [2]\n",
    "* A `NormalParamExtractor` module that will split this output on two chunks, a mean and a standard deviation of size [1];\n",
    "* A `ProbabilisticActor` that will read those parameters as in_keys, create a distribution with them and populate our tensordict with samples and log-probabilities.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5c659b4a2210d2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e95815d7c94c401a"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        loc: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        sample_log_prob: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        scale: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch.distributions import Normal\n",
    "from torchrl.modules import ProbabilisticActor\n",
    "\n",
    "backbone = MLP(in_features=3, out_features=2, device=fetch_device())\n",
    "extractor = NormalParamExtractor()\n",
    "module = torch.nn.Sequential(backbone, extractor)\n",
    "td_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "policy = ProbabilisticActor(\n",
    "    td_module,\n",
    "    in_keys=[\"loc\", \"scale\"],    # Parameters of Gaussian distribution.\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=Normal,\n",
    "    return_log_prob=True,\n",
    ")\n",
    "\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)\n",
    "\n",
    "# Since we asked for it during the construction of the actor, the log-probability of the actions given the distribution at that time is also written. This is necessary for sota-implementations like PPO.\n",
    "\n",
    "# The parameters of the distribution are returned within the output tensordict too under the \"loc\" and \"scale\" entries."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:14:32.353244200Z",
     "start_time": "2024-04-13T14:14:32.259012Z"
    }
   },
   "id": "d8537d24ec453f40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can control the sampling of the action to use the expected value or other properties of the distribution instead of using random samples if your application requires it. This can be controlled via the set_exploration_type() function:\n",
    "```\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # takes the mean as action\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Samples actions according to the dist\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c549dd73b58f599"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Epsilon-Greedy Action Selection for Value-Based algorithms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f181e0cf5fb0dc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic policies like this somewhat naturally trade off exploration and exploitation, but deterministic policies won’t. Fortunately, TorchRL can also palliate to this with its exploration modules. We will take the example of the EGreedyModule exploration module (check also AdditiveGaussianWrapper and OrnsteinUhlenbeckProcessWrapper). To see this module in action, let’s revert to a deterministic policy:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff6c2ead03d6d93"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.modules import EGreedyModule\n",
    "\n",
    "policy = Actor(MLP(3, 1, num_cells=[32, 64], device=fetch_device()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:15:24.146399200Z",
     "start_time": "2024-04-13T14:15:24.138252800Z"
    }
   },
   "id": "ead20be6768a190e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our $\\epsilon$-greedy exploration module will usually be customized with a number of annealing frames and an initial value for the parameter. A value of means that every action taken is random, while means that there is no exploration at all. To anneal (i.e., decrease) the exploration factor, a call to step() is required (see the last tutorial for an example).\n",
    "\n",
    "Because it must be able to sample random actions in the action space, the EGreedyModule must be equipped with the action_space from the environment to know what strategy to use to sample actions randomly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faafddad71317737"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "exploration_module = EGreedyModule(\n",
    "    spec=env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:15:40.903853400Z",
     "start_time": "2024-04-13T14:15:40.864439300Z"
    }
   },
   "id": "deea37db1c0f6ec9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To build our explorative policy, we only had to concatenate the deterministic policy module with the exploration module within a TensorDictSequential module (which is the analogous to Sequential in the tensordict realm)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7acade2f1dc835d"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
    "\n",
    "with set_exploration_type(ExplorationType.MEAN):\n",
    "    # Turns off exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Turns on exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:17:00.820431800Z",
     "start_time": "2024-04-13T14:17:00.756683700Z"
    }
   },
   "id": "9d966a3d6090c062"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Value actors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349ae9ff4e285d2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In some settings, the policy isn’t a standalone module but is constructed on top of another module. This is the case with Q-Value actors. In short, these actors require an estimate of the action value (most of the time discrete) and will greedily pick up the action with the highest value. In some settings (finite discrete action space and finite discrete state space), one can just store a 2D table of state-action pairs and pick up the action with the highest value. The innovation brought by DQN was to scale this up to continuous state spaces by utilizing a neural network to encode for the Q(s, a) value map. Let’s consider another environment with a discrete action space for a clearer understanding:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adcfe98832b647d4"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotDiscreteTensorSpec(\n",
      "    shape=torch.Size([2]),\n",
      "    space=DiscreteBox(n=2),\n",
      "    device=cuda,\n",
      "    dtype=torch.int64,\n",
      "    domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(\"CartPole-v1\", device=fetch_device())\n",
    "print(env.action_spec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:19:30.190659600Z",
     "start_time": "2024-04-13T14:19:30.151643400Z"
    }
   },
   "id": "51eb306747f53dba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We build a value network that produces one value per action when it reads a state from the environment:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d6dae816bde4bd"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "value_net = TensorDictModule(\n",
    "    MLP(out_features=num_actions, num_cells=[32, 32], device=fetch_device()),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_value\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:21:17.343111100Z",
     "start_time": "2024-04-13T14:21:17.343111100Z"
    }
   },
   "id": "1fd8d4eb2ce04fdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily build our Q-Value actor by adding a QValueModule after our value network:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e65dccbbc5d10974"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from torchrl.modules import QValueModule\n",
    "\n",
    "policy = TensorDictSequential(\n",
    "    value_net,  # writes action values in our tensordict\n",
    "    QValueModule(\n",
    "        action_space=env.action_spec\n",
    "    ),  # Reads the \"action_value\" entry by default\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:21:18.667829300Z",
     "start_time": "2024-04-13T14:21:18.654818500Z"
    }
   },
   "id": "785e15cbe547640c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s check it out! We run the policy for a couple of steps and look at the output. We should find an \"action_value\" as well as a \"chosen_action_value\" entries in the rollout that we obtain:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc622aaccb9b9c7c"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 2]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        action_value: Tensor(shape=torch.Size([3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=cuda,\n",
      "            is_shared=True),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=3, policy=policy)\n",
    "print(rollout)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:21:20.347470100Z",
     "start_time": "2024-04-13T14:21:20.256080600Z"
    }
   },
   "id": "4a9993050fe6726e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because it relies on the argmax operator, this policy is deterministic. During data collection, we will need to explore the environment. For that, we are using the EGreedyModule once again:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4389d6f0da42ac32"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "policy_explore = TensorDictSequential(policy, EGreedyModule(env.action_spec))\n",
    "\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T14:22:02.273348600Z",
     "start_time": "2024-04-13T14:22:02.230567300Z"
    }
   },
   "id": "bfb850c3e9486c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b316530c2f6af509"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
