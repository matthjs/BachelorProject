{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T18:37:20.543317500Z",
     "start_time": "2024-06-23T18:37:15.201664800Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultivariateNormal' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 75\u001B[0m\n\u001B[1;32m     73\u001B[0m dataset_sizes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m500\u001B[39m, \u001B[38;5;241m1000\u001B[39m]  \u001B[38;5;66;03m# Example dataset sizes\u001B[39;00m\n\u001B[1;32m     74\u001B[0m inducing_points_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m256\u001B[39m]  \u001B[38;5;66;03m# Example inducing points\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m time_results, memory_results \u001B[38;5;241m=\u001B[39m \u001B[43mbenchmark_deepgp_gpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_sizes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minducing_points_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Shutdown NVML\u001B[39;00m\n\u001B[1;32m     78\u001B[0m nvmlShutdown()\n",
      "Cell \u001B[0;32mIn[1], line 56\u001B[0m, in \u001B[0;36mbenchmark_deepgp_gpu\u001B[0;34m(dataset_sizes, inducing_points_list)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     55\u001B[0m     x_batch, y_batch \u001B[38;5;241m=\u001B[39m x_batch\u001B[38;5;241m.\u001B[39mto(device), y_batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 56\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n",
      "File \u001B[0;32m~/bsc/BachelorProject/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3355\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[1;32m   3352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3353\u001B[0m         mse_loss, (\u001B[38;5;28minput\u001B[39m, target), \u001B[38;5;28minput\u001B[39m, target, size_average\u001B[38;5;241m=\u001B[39msize_average, reduce\u001B[38;5;241m=\u001B[39mreduce, reduction\u001B[38;5;241m=\u001B[39mreduction\n\u001B[1;32m   3354\u001B[0m     )\n\u001B[0;32m-> 3355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()):\n\u001B[1;32m   3356\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   3357\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3360\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   3361\u001B[0m     )\n\u001B[1;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'MultivariateNormal' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from gp.fitgp import GPFitter\n",
    "from gp.deepgp import DeepGPModel\n",
    "import pynvml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlShutdown\n",
    "\n",
    "# Initialize NVML\n",
    "nvmlInit()\n",
    "handle = nvmlDeviceGetHandleByIndex(0)  # Assuming GPU 0\n",
    "def gpu_memory_usage():\n",
    "    \"\"\"\n",
    "    Assumes only one GPU is available.\n",
    "    Uses the NVIDIA Management Library to return GPU memory usage information.\n",
    "    :return: Returns the total amount of VRAM available on the GPU and the VRAM usage.\n",
    "    \"\"\"\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming you have only one GPU\n",
    "    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    total_memory = mem_info.total  # in bytes\n",
    "    used_memory = mem_info.used  # in bytes\n",
    "    pynvml.nvmlShutdown()\n",
    "    return total_memory / (1024 ** 3), used_memory / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "# Define a function to create and train DeepGPModel on GPU\n",
    "def benchmark_deepgp_gpu(dataset_sizes, inducing_points_list, dim=4):\n",
    "    time_results = np.zeros((len(dataset_sizes), len(inducing_points_list)))\n",
    "    memory_results = np.zeros((len(dataset_sizes), len(inducing_points_list)))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for i, dataset_size in enumerate(dataset_sizes):\n",
    "        train_x = torch.randn(dataset_size, dim, device=device)\n",
    "        train_y = torch.randn(dataset_size, 1, device=device)\n",
    "\n",
    "        for j, num_inducing_points in enumerate(inducing_points_list):\n",
    "            model = DeepGPModel(\n",
    "                train_x_shape=train_x.shape,\n",
    "                hidden_layers_config=[{'output_dims': None, 'mean_type': 'constant'}],  # Example config\n",
    "                num_inducing_points=num_inducing_points\n",
    "            ).to(device)\n",
    "\n",
    "            train_dataset = TensorDataset(train_x, train_y)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "            # Profile memory usage during training\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            start_time = time.time()\n",
    "            GPFitter()(model,\n",
    "                       train_x,\n",
    "                       train_y,\n",
    "                       \"deep_gp\",\n",
    "                        batch_size=128,\n",
    "                       num_epochs=1)\n",
    "            for epoch in range(10):\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                    loss = torch.nn.functional.mse_loss(model(x_batch), y_batch)\n",
    "                    loss.backward()\n",
    "                    # Update parameters\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "\n",
    "            memory_usage = torch.cuda.max_memory_allocated() / 1024 ** 2  # Peak memory usage in MiB\n",
    "\n",
    "            # Record results\n",
    "            time_results[i, j] = execution_time\n",
    "            memory_results[i, j] = memory_usage\n",
    "\n",
    "    return time_results, memory_results\n",
    "\n",
    "# Example usage\n",
    "dataset_sizes = [100, 500, 1000]  # Example dataset sizes\n",
    "inducing_points_list = [64, 128, 256]  # Example inducing points\n",
    "time_results, memory_results = benchmark_deepgp_gpu(dataset_sizes, inducing_points_list)\n",
    "\n",
    "# Shutdown NVML\n",
    "nvmlShutdown()\n",
    "\n",
    "# Plotting results (time)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for j, num_inducing_points in enumerate(inducing_points_list):\n",
    "    plt.plot(dataset_sizes, time_results[:, j], marker='o', label=f'Inducing Points: {num_inducing_points}')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Time vs. Dataset Size for Different Inducing Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting results (memory)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for j, num_inducing_points in enumerate(inducing_points_list):\n",
    "    plt.plot(dataset_sizes, memory_results[:, j], marker='o', label=f'Inducing Points: {num_inducing_points}')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Memory Usage (MiB)')\n",
    "plt.title('Peak Memory Usage vs. Dataset Size for Different Inducing Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
