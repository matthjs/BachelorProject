agent_id,agent_type,hyperparams,num_episodes (train),avg return train,stdev return train,execution time (sec) train,VRAM usage (GB) train,Energy usage (J) train,p_val_cuml_return_train_>,W-statistic_cuml,num_episodes (eval),avg return eval,stdev return eval,execution time (sec) eval,VRAM usage (GB) eval,Energy usage (J) eval,p_val_return_eval_>,W-statistic_return_eval
random,random,{},30.0,23.5,14.978,0.917,0.0,627.748,0.5,0.0,10.0,22.5,11.168,0.003,0.0,0.0,0.5,0.0
sb_dqn_1,sb_dqn,"{'policy': 'MlpPolicy', 'learning_rate': 0.0023, 'batch_size': 64, 'buffer_size': 100000, 'learning_starts': 1000, 'gamma': 0.99, 'target_update_interval': 10, 'train_freq': 256, 'gradient_steps': 128, 'exploration_fraction': 0.16, 'exploration_final_eps': 0.04, 'policy_kwargs': {'net_arch': [256, 256]}}",30.0,27.867,17.683,0.102,0.0,53.699,0.0,5.034,10.0,20.9,9.86,0.949,0.385,69.608,0.619,-0.302
